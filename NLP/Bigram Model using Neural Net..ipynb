{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eeb8b61",
   "metadata": {},
   "source": [
    "<h1> Cast the problem of bi-gram character level language modeling into the neural network framework</h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "199f5138",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ce19228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the dictionary to save the pair of token of words \n",
    "b={}\n",
    "for w in words[:]:\n",
    "    ch = ['<S>']+ list(w) + ['<E>']\n",
    "    for ch1, ch2 in zip(ch, ch[1:]):\n",
    "        bigram = (ch1, ch2)\n",
    "        b[bigram] = b.get(bigram , 0)+ 1\n",
    "#         print(ch1, ch2)\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a562103a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('n', '<E>'), 6763),\n",
       " (('a', '<E>'), 6640),\n",
       " (('a', 'n'), 5438),\n",
       " (('<S>', 'a'), 4410),\n",
       " (('e', '<E>'), 3983),\n",
       " (('a', 'r'), 3264),\n",
       " (('e', 'l'), 3248),\n",
       " (('r', 'i'), 3033),\n",
       " (('n', 'a'), 2977),\n",
       " (('<S>', 'k'), 2963),\n",
       " (('l', 'e'), 2921),\n",
       " (('e', 'n'), 2675),\n",
       " (('l', 'a'), 2623),\n",
       " (('m', 'a'), 2590),\n",
       " (('<S>', 'm'), 2538),\n",
       " (('a', 'l'), 2528),\n",
       " (('i', '<E>'), 2489),\n",
       " (('l', 'i'), 2480),\n",
       " (('i', 'a'), 2445),\n",
       " (('<S>', 'j'), 2422),\n",
       " (('o', 'n'), 2411),\n",
       " (('h', '<E>'), 2409),\n",
       " (('r', 'a'), 2356),\n",
       " (('a', 'h'), 2332),\n",
       " (('h', 'a'), 2244),\n",
       " (('y', 'a'), 2143),\n",
       " (('i', 'n'), 2126),\n",
       " (('<S>', 's'), 2055),\n",
       " (('a', 'y'), 2050),\n",
       " (('y', '<E>'), 2007),\n",
       " (('e', 'r'), 1958),\n",
       " (('n', 'n'), 1906),\n",
       " (('y', 'n'), 1826),\n",
       " (('k', 'a'), 1731),\n",
       " (('n', 'i'), 1725),\n",
       " (('r', 'e'), 1697),\n",
       " (('<S>', 'd'), 1690),\n",
       " (('i', 'e'), 1653),\n",
       " (('a', 'i'), 1650),\n",
       " (('<S>', 'r'), 1639),\n",
       " (('a', 'm'), 1634),\n",
       " (('l', 'y'), 1588),\n",
       " (('<S>', 'l'), 1572),\n",
       " (('<S>', 'c'), 1542),\n",
       " (('<S>', 'e'), 1531),\n",
       " (('j', 'a'), 1473),\n",
       " (('r', '<E>'), 1377),\n",
       " (('n', 'e'), 1359),\n",
       " (('l', 'l'), 1345),\n",
       " (('i', 'l'), 1345),\n",
       " (('i', 's'), 1316),\n",
       " (('l', '<E>'), 1314),\n",
       " (('<S>', 't'), 1308),\n",
       " (('<S>', 'b'), 1306),\n",
       " (('d', 'a'), 1303),\n",
       " (('s', 'h'), 1285),\n",
       " (('d', 'e'), 1283),\n",
       " (('e', 'e'), 1271),\n",
       " (('m', 'i'), 1256),\n",
       " (('s', 'a'), 1201),\n",
       " (('s', '<E>'), 1169),\n",
       " (('<S>', 'n'), 1146),\n",
       " (('a', 's'), 1118),\n",
       " (('y', 'l'), 1104),\n",
       " (('e', 'y'), 1070),\n",
       " (('o', 'r'), 1059),\n",
       " (('a', 'd'), 1042),\n",
       " (('t', 'a'), 1027),\n",
       " (('<S>', 'z'), 929),\n",
       " (('v', 'i'), 911),\n",
       " (('k', 'e'), 895),\n",
       " (('s', 'e'), 884),\n",
       " (('<S>', 'h'), 874),\n",
       " (('r', 'o'), 869),\n",
       " (('e', 's'), 861),\n",
       " (('z', 'a'), 860),\n",
       " (('o', '<E>'), 855),\n",
       " (('i', 'r'), 849),\n",
       " (('b', 'r'), 842),\n",
       " (('a', 'v'), 834),\n",
       " (('m', 'e'), 818),\n",
       " (('e', 'i'), 818),\n",
       " (('c', 'a'), 815),\n",
       " (('i', 'y'), 779),\n",
       " (('r', 'y'), 773),\n",
       " (('e', 'm'), 769),\n",
       " (('s', 't'), 765),\n",
       " (('h', 'i'), 729),\n",
       " (('t', 'e'), 716),\n",
       " (('n', 'd'), 704),\n",
       " (('l', 'o'), 692),\n",
       " (('a', 'e'), 692),\n",
       " (('a', 't'), 687),\n",
       " (('s', 'i'), 684),\n",
       " (('e', 'a'), 679),\n",
       " (('d', 'i'), 674),\n",
       " (('h', 'e'), 674),\n",
       " (('<S>', 'g'), 669),\n",
       " (('t', 'o'), 667),\n",
       " (('c', 'h'), 664),\n",
       " (('b', 'e'), 655),\n",
       " (('t', 'h'), 647),\n",
       " (('v', 'a'), 642),\n",
       " (('o', 'l'), 619),\n",
       " (('<S>', 'i'), 591),\n",
       " (('i', 'o'), 588),\n",
       " (('e', 't'), 580),\n",
       " (('v', 'e'), 568),\n",
       " (('a', 'k'), 568),\n",
       " (('a', 'a'), 556),\n",
       " (('c', 'e'), 551),\n",
       " (('a', 'b'), 541),\n",
       " (('i', 't'), 541),\n",
       " (('<S>', 'y'), 535),\n",
       " (('t', 'i'), 532),\n",
       " (('s', 'o'), 531),\n",
       " (('m', '<E>'), 516),\n",
       " (('d', '<E>'), 516),\n",
       " (('<S>', 'p'), 515),\n",
       " (('i', 'c'), 509),\n",
       " (('k', 'i'), 509),\n",
       " (('o', 's'), 504),\n",
       " (('n', 'o'), 496),\n",
       " (('t', '<E>'), 483),\n",
       " (('j', 'o'), 479),\n",
       " (('u', 's'), 474),\n",
       " (('a', 'c'), 470),\n",
       " (('n', 'y'), 465),\n",
       " (('e', 'v'), 463),\n",
       " (('s', 's'), 461),\n",
       " (('m', 'o'), 452),\n",
       " (('i', 'k'), 445),\n",
       " (('n', 't'), 443),\n",
       " (('i', 'd'), 440),\n",
       " (('j', 'e'), 440),\n",
       " (('a', 'z'), 435),\n",
       " (('i', 'g'), 428),\n",
       " (('i', 'm'), 427),\n",
       " (('r', 'r'), 425),\n",
       " (('d', 'r'), 424),\n",
       " (('<S>', 'f'), 417),\n",
       " (('u', 'r'), 414),\n",
       " (('r', 'l'), 413),\n",
       " (('y', 's'), 401),\n",
       " (('<S>', 'o'), 394),\n",
       " (('e', 'd'), 384),\n",
       " (('a', 'u'), 381),\n",
       " (('c', 'o'), 380),\n",
       " (('k', 'y'), 379),\n",
       " (('d', 'o'), 378),\n",
       " (('<S>', 'v'), 376),\n",
       " (('t', 't'), 374),\n",
       " (('z', 'e'), 373),\n",
       " (('z', 'i'), 364),\n",
       " (('k', '<E>'), 363),\n",
       " (('g', 'h'), 360),\n",
       " (('t', 'r'), 352),\n",
       " (('k', 'o'), 344),\n",
       " (('t', 'y'), 341),\n",
       " (('g', 'e'), 334),\n",
       " (('g', 'a'), 330),\n",
       " (('l', 'u'), 324),\n",
       " (('b', 'a'), 321),\n",
       " (('d', 'y'), 317),\n",
       " (('c', 'k'), 316),\n",
       " (('<S>', 'w'), 307),\n",
       " (('k', 'h'), 307),\n",
       " (('u', 'l'), 301),\n",
       " (('y', 'e'), 301),\n",
       " (('y', 'r'), 291),\n",
       " (('m', 'y'), 287),\n",
       " (('h', 'o'), 287),\n",
       " (('w', 'a'), 280),\n",
       " (('s', 'l'), 279),\n",
       " (('n', 's'), 278),\n",
       " (('i', 'z'), 277),\n",
       " (('u', 'n'), 275),\n",
       " (('o', 'u'), 275),\n",
       " (('n', 'g'), 273),\n",
       " (('y', 'd'), 272),\n",
       " (('c', 'i'), 271),\n",
       " (('y', 'o'), 271),\n",
       " (('i', 'v'), 269),\n",
       " (('e', 'o'), 269),\n",
       " (('o', 'm'), 261),\n",
       " (('r', 'u'), 252),\n",
       " (('f', 'a'), 242),\n",
       " (('b', 'i'), 217),\n",
       " (('s', 'y'), 215),\n",
       " (('n', 'c'), 213),\n",
       " (('h', 'y'), 213),\n",
       " (('p', 'a'), 209),\n",
       " (('r', 't'), 208),\n",
       " (('q', 'u'), 206),\n",
       " (('p', 'h'), 204),\n",
       " (('h', 'r'), 204),\n",
       " (('j', 'u'), 202),\n",
       " (('g', 'r'), 201),\n",
       " (('p', 'e'), 197),\n",
       " (('n', 'l'), 195),\n",
       " (('y', 'i'), 192),\n",
       " (('g', 'i'), 190),\n",
       " (('o', 'd'), 190),\n",
       " (('r', 's'), 190),\n",
       " (('r', 'd'), 187),\n",
       " (('h', 'l'), 185),\n",
       " (('s', 'u'), 185),\n",
       " (('a', 'x'), 182),\n",
       " (('e', 'z'), 181),\n",
       " (('e', 'k'), 178),\n",
       " (('o', 'v'), 176),\n",
       " (('a', 'j'), 175),\n",
       " (('o', 'h'), 171),\n",
       " (('u', 'e'), 169),\n",
       " (('m', 'm'), 168),\n",
       " (('a', 'g'), 168),\n",
       " (('h', 'u'), 166),\n",
       " (('x', '<E>'), 164),\n",
       " (('u', 'a'), 163),\n",
       " (('r', 'm'), 162),\n",
       " (('a', 'w'), 161),\n",
       " (('f', 'i'), 160),\n",
       " (('z', '<E>'), 160),\n",
       " (('u', '<E>'), 155),\n",
       " (('u', 'm'), 154),\n",
       " (('e', 'c'), 153),\n",
       " (('v', 'o'), 153),\n",
       " (('e', 'h'), 152),\n",
       " (('p', 'r'), 151),\n",
       " (('d', 'd'), 149),\n",
       " (('o', 'a'), 149),\n",
       " (('w', 'e'), 149),\n",
       " (('w', 'i'), 148),\n",
       " (('y', 'm'), 148),\n",
       " (('z', 'y'), 147),\n",
       " (('n', 'z'), 145),\n",
       " (('y', 'u'), 141),\n",
       " (('r', 'n'), 140),\n",
       " (('o', 'b'), 140),\n",
       " (('k', 'l'), 139),\n",
       " (('m', 'u'), 139),\n",
       " (('l', 'd'), 138),\n",
       " (('h', 'n'), 138),\n",
       " (('u', 'd'), 136),\n",
       " (('<S>', 'x'), 134),\n",
       " (('t', 'l'), 134),\n",
       " (('a', 'f'), 134),\n",
       " (('o', 'e'), 132),\n",
       " (('e', 'x'), 132),\n",
       " (('e', 'g'), 125),\n",
       " (('f', 'e'), 123),\n",
       " (('z', 'l'), 123),\n",
       " (('u', 'i'), 121),\n",
       " (('v', 'y'), 121),\n",
       " (('e', 'b'), 121),\n",
       " (('r', 'h'), 121),\n",
       " (('j', 'i'), 119),\n",
       " (('o', 't'), 118),\n",
       " (('d', 'h'), 118),\n",
       " (('h', 'm'), 117),\n",
       " (('c', 'l'), 116),\n",
       " (('o', 'o'), 115),\n",
       " (('y', 'c'), 115),\n",
       " (('o', 'w'), 114),\n",
       " (('o', 'c'), 114),\n",
       " (('f', 'r'), 114),\n",
       " (('b', '<E>'), 114),\n",
       " (('m', 'b'), 112),\n",
       " (('z', 'o'), 110),\n",
       " (('i', 'b'), 110),\n",
       " (('i', 'u'), 109),\n",
       " (('k', 'r'), 109),\n",
       " (('g', '<E>'), 108),\n",
       " (('y', 'v'), 106),\n",
       " (('t', 'z'), 105),\n",
       " (('b', 'o'), 105),\n",
       " (('c', 'y'), 104),\n",
       " (('y', 't'), 104),\n",
       " (('u', 'b'), 103),\n",
       " (('u', 'c'), 103),\n",
       " (('x', 'a'), 103),\n",
       " (('b', 'l'), 103),\n",
       " (('o', 'y'), 103),\n",
       " (('x', 'i'), 102),\n",
       " (('i', 'f'), 101),\n",
       " (('r', 'c'), 99),\n",
       " (('c', '<E>'), 97),\n",
       " (('m', 'r'), 97),\n",
       " (('n', 'u'), 96),\n",
       " (('o', 'p'), 95),\n",
       " (('i', 'h'), 95),\n",
       " (('k', 's'), 95),\n",
       " (('l', 's'), 94),\n",
       " (('u', 'k'), 93),\n",
       " (('<S>', 'q'), 92),\n",
       " (('d', 'u'), 92),\n",
       " (('s', 'm'), 90),\n",
       " (('r', 'k'), 90),\n",
       " (('i', 'x'), 89),\n",
       " (('v', '<E>'), 88),\n",
       " (('y', 'k'), 86),\n",
       " (('u', 'w'), 86),\n",
       " (('g', 'u'), 85),\n",
       " (('b', 'y'), 83),\n",
       " (('e', 'p'), 83),\n",
       " (('g', 'o'), 83),\n",
       " (('s', 'k'), 82),\n",
       " (('u', 't'), 82),\n",
       " (('a', 'p'), 82),\n",
       " (('e', 'f'), 82),\n",
       " (('i', 'i'), 82),\n",
       " (('r', 'v'), 80),\n",
       " (('f', '<E>'), 80),\n",
       " (('t', 'u'), 78),\n",
       " (('y', 'z'), 78),\n",
       " (('<S>', 'u'), 78),\n",
       " (('l', 't'), 77),\n",
       " (('r', 'g'), 76),\n",
       " (('c', 'r'), 76),\n",
       " (('i', 'j'), 76),\n",
       " (('w', 'y'), 73),\n",
       " (('z', 'u'), 73),\n",
       " (('l', 'v'), 72),\n",
       " (('h', 't'), 71),\n",
       " (('j', '<E>'), 71),\n",
       " (('x', 't'), 70),\n",
       " (('o', 'i'), 69),\n",
       " (('e', 'u'), 69),\n",
       " (('o', 'k'), 68),\n",
       " (('b', 'd'), 65),\n",
       " (('a', 'o'), 63),\n",
       " (('p', 'i'), 61),\n",
       " (('s', 'c'), 60),\n",
       " (('d', 'l'), 60),\n",
       " (('l', 'm'), 60),\n",
       " (('a', 'q'), 60),\n",
       " (('f', 'o'), 60),\n",
       " (('p', 'o'), 59),\n",
       " (('n', 'k'), 58),\n",
       " (('w', 'n'), 58),\n",
       " (('u', 'h'), 58),\n",
       " (('e', 'j'), 55),\n",
       " (('n', 'v'), 55),\n",
       " (('s', 'r'), 55),\n",
       " (('o', 'z'), 54),\n",
       " (('i', 'p'), 53),\n",
       " (('l', 'b'), 52),\n",
       " (('i', 'q'), 52),\n",
       " (('w', '<E>'), 51),\n",
       " (('m', 'c'), 51),\n",
       " (('s', 'p'), 51),\n",
       " (('e', 'w'), 50),\n",
       " (('k', 'u'), 50),\n",
       " (('v', 'r'), 48),\n",
       " (('u', 'g'), 47),\n",
       " (('o', 'x'), 45),\n",
       " (('u', 'z'), 45),\n",
       " (('z', 'z'), 45),\n",
       " (('j', 'h'), 45),\n",
       " (('b', 'u'), 45),\n",
       " (('o', 'g'), 44),\n",
       " (('n', 'r'), 44),\n",
       " (('f', 'f'), 44),\n",
       " (('n', 'j'), 44),\n",
       " (('z', 'h'), 43),\n",
       " (('c', 'c'), 42),\n",
       " (('r', 'b'), 41),\n",
       " (('x', 'o'), 41),\n",
       " (('b', 'h'), 41),\n",
       " (('p', 'p'), 39),\n",
       " (('x', 'l'), 39),\n",
       " (('h', 'v'), 39),\n",
       " (('b', 'b'), 38),\n",
       " (('m', 'p'), 38),\n",
       " (('x', 'x'), 38),\n",
       " (('u', 'v'), 37),\n",
       " (('x', 'e'), 36),\n",
       " (('w', 'o'), 36),\n",
       " (('c', 't'), 35),\n",
       " (('z', 'm'), 35),\n",
       " (('t', 's'), 35),\n",
       " (('m', 's'), 35),\n",
       " (('c', 'u'), 35),\n",
       " (('o', 'f'), 34),\n",
       " (('u', 'x'), 34),\n",
       " (('k', 'w'), 34),\n",
       " (('p', '<E>'), 33),\n",
       " (('g', 'l'), 32),\n",
       " (('z', 'r'), 32),\n",
       " (('d', 'n'), 31),\n",
       " (('g', 't'), 31),\n",
       " (('g', 'y'), 31),\n",
       " (('h', 's'), 31),\n",
       " (('x', 's'), 31),\n",
       " (('g', 's'), 30),\n",
       " (('x', 'y'), 30),\n",
       " (('y', 'g'), 30),\n",
       " (('d', 'm'), 30),\n",
       " (('d', 's'), 29),\n",
       " (('h', 'k'), 29),\n",
       " (('y', 'x'), 28),\n",
       " (('q', '<E>'), 28),\n",
       " (('g', 'n'), 27),\n",
       " (('y', 'b'), 27),\n",
       " (('g', 'w'), 26),\n",
       " (('n', 'h'), 26),\n",
       " (('k', 'n'), 26),\n",
       " (('g', 'g'), 25),\n",
       " (('d', 'g'), 25),\n",
       " (('l', 'c'), 25),\n",
       " (('r', 'j'), 25),\n",
       " (('w', 'u'), 25),\n",
       " (('l', 'k'), 24),\n",
       " (('m', 'd'), 24),\n",
       " (('s', 'w'), 24),\n",
       " (('s', 'n'), 24),\n",
       " (('h', 'd'), 24),\n",
       " (('w', 'h'), 23),\n",
       " (('y', 'j'), 23),\n",
       " (('y', 'y'), 23),\n",
       " (('r', 'z'), 23),\n",
       " (('d', 'w'), 23),\n",
       " (('w', 'r'), 22),\n",
       " (('t', 'n'), 22),\n",
       " (('l', 'f'), 22),\n",
       " (('y', 'h'), 22),\n",
       " (('r', 'w'), 21),\n",
       " (('s', 'b'), 21),\n",
       " (('m', 'n'), 20),\n",
       " (('f', 'l'), 20),\n",
       " (('w', 's'), 20),\n",
       " (('k', 'k'), 20),\n",
       " (('h', 'z'), 20),\n",
       " (('g', 'd'), 19),\n",
       " (('l', 'h'), 19),\n",
       " (('n', 'm'), 19),\n",
       " (('x', 'z'), 19),\n",
       " (('u', 'f'), 19),\n",
       " (('f', 't'), 18),\n",
       " (('l', 'r'), 18),\n",
       " (('p', 't'), 17),\n",
       " (('t', 'c'), 17),\n",
       " (('k', 't'), 17),\n",
       " (('d', 'v'), 17),\n",
       " (('u', 'p'), 16),\n",
       " (('p', 'l'), 16),\n",
       " (('l', 'w'), 16),\n",
       " (('p', 's'), 16),\n",
       " (('o', 'j'), 16),\n",
       " (('r', 'q'), 16),\n",
       " (('y', 'p'), 15),\n",
       " (('l', 'p'), 15),\n",
       " (('t', 'v'), 15),\n",
       " (('r', 'p'), 14),\n",
       " (('l', 'n'), 14),\n",
       " (('e', 'q'), 14),\n",
       " (('f', 'y'), 14),\n",
       " (('s', 'v'), 14),\n",
       " (('u', 'j'), 14),\n",
       " (('v', 'l'), 14),\n",
       " (('q', 'a'), 13),\n",
       " (('u', 'y'), 13),\n",
       " (('q', 'i'), 13),\n",
       " (('w', 'l'), 13),\n",
       " (('p', 'y'), 12),\n",
       " (('y', 'f'), 12),\n",
       " (('c', 'q'), 11),\n",
       " (('j', 'r'), 11),\n",
       " (('n', 'w'), 11),\n",
       " (('n', 'f'), 11),\n",
       " (('t', 'w'), 11),\n",
       " (('m', 'z'), 11),\n",
       " (('u', 'o'), 10),\n",
       " (('f', 'u'), 10),\n",
       " (('l', 'z'), 10),\n",
       " (('h', 'w'), 10),\n",
       " (('u', 'q'), 10),\n",
       " (('j', 'y'), 10),\n",
       " (('s', 'z'), 10),\n",
       " (('s', 'd'), 9),\n",
       " (('j', 'l'), 9),\n",
       " (('d', 'j'), 9),\n",
       " (('k', 'm'), 9),\n",
       " (('r', 'f'), 9),\n",
       " (('h', 'j'), 9),\n",
       " (('v', 'n'), 8),\n",
       " (('n', 'b'), 8),\n",
       " (('i', 'w'), 8),\n",
       " (('h', 'b'), 8),\n",
       " (('b', 's'), 8),\n",
       " (('w', 't'), 8),\n",
       " (('w', 'd'), 8),\n",
       " (('v', 'v'), 7),\n",
       " (('v', 'u'), 7),\n",
       " (('j', 's'), 7),\n",
       " (('m', 'j'), 7),\n",
       " (('f', 's'), 6),\n",
       " (('l', 'g'), 6),\n",
       " (('l', 'j'), 6),\n",
       " (('j', 'w'), 6),\n",
       " (('n', 'x'), 6),\n",
       " (('y', 'q'), 6),\n",
       " (('w', 'k'), 6),\n",
       " (('g', 'm'), 6),\n",
       " (('x', 'u'), 5),\n",
       " (('m', 'h'), 5),\n",
       " (('m', 'l'), 5),\n",
       " (('j', 'm'), 5),\n",
       " (('c', 's'), 5),\n",
       " (('j', 'v'), 5),\n",
       " (('n', 'p'), 5),\n",
       " (('d', 'f'), 5),\n",
       " (('x', 'd'), 5),\n",
       " (('z', 'b'), 4),\n",
       " (('f', 'n'), 4),\n",
       " (('x', 'c'), 4),\n",
       " (('m', 't'), 4),\n",
       " (('t', 'm'), 4),\n",
       " (('z', 'n'), 4),\n",
       " (('z', 't'), 4),\n",
       " (('p', 'u'), 4),\n",
       " (('c', 'z'), 4),\n",
       " (('b', 'n'), 4),\n",
       " (('z', 's'), 4),\n",
       " (('f', 'w'), 4),\n",
       " (('d', 't'), 4),\n",
       " (('j', 'd'), 4),\n",
       " (('j', 'c'), 4),\n",
       " (('y', 'w'), 4),\n",
       " (('v', 'k'), 3),\n",
       " (('x', 'w'), 3),\n",
       " (('t', 'j'), 3),\n",
       " (('c', 'j'), 3),\n",
       " (('q', 'w'), 3),\n",
       " (('g', 'b'), 3),\n",
       " (('o', 'q'), 3),\n",
       " (('r', 'x'), 3),\n",
       " (('d', 'c'), 3),\n",
       " (('g', 'j'), 3),\n",
       " (('x', 'f'), 3),\n",
       " (('z', 'w'), 3),\n",
       " (('d', 'k'), 3),\n",
       " (('u', 'u'), 3),\n",
       " (('m', 'v'), 3),\n",
       " (('c', 'x'), 3),\n",
       " (('l', 'q'), 3),\n",
       " (('p', 'b'), 2),\n",
       " (('t', 'g'), 2),\n",
       " (('q', 's'), 2),\n",
       " (('t', 'x'), 2),\n",
       " (('f', 'k'), 2),\n",
       " (('b', 't'), 2),\n",
       " (('j', 'n'), 2),\n",
       " (('k', 'c'), 2),\n",
       " (('z', 'k'), 2),\n",
       " (('s', 'j'), 2),\n",
       " (('s', 'f'), 2),\n",
       " (('z', 'j'), 2),\n",
       " (('n', 'q'), 2),\n",
       " (('f', 'z'), 2),\n",
       " (('h', 'g'), 2),\n",
       " (('w', 'w'), 2),\n",
       " (('k', 'j'), 2),\n",
       " (('j', 'k'), 2),\n",
       " (('w', 'm'), 2),\n",
       " (('z', 'c'), 2),\n",
       " (('z', 'v'), 2),\n",
       " (('w', 'f'), 2),\n",
       " (('q', 'm'), 2),\n",
       " (('k', 'z'), 2),\n",
       " (('j', 'j'), 2),\n",
       " (('z', 'p'), 2),\n",
       " (('j', 't'), 2),\n",
       " (('k', 'b'), 2),\n",
       " (('m', 'w'), 2),\n",
       " (('h', 'f'), 2),\n",
       " (('c', 'g'), 2),\n",
       " (('t', 'f'), 2),\n",
       " (('h', 'c'), 2),\n",
       " (('q', 'o'), 2),\n",
       " (('k', 'd'), 2),\n",
       " (('k', 'v'), 2),\n",
       " (('s', 'g'), 2),\n",
       " (('z', 'd'), 2),\n",
       " (('q', 'r'), 1),\n",
       " (('d', 'z'), 1),\n",
       " (('p', 'j'), 1),\n",
       " (('q', 'l'), 1),\n",
       " (('p', 'f'), 1),\n",
       " (('q', 'e'), 1),\n",
       " (('b', 'c'), 1),\n",
       " (('c', 'd'), 1),\n",
       " (('m', 'f'), 1),\n",
       " (('p', 'n'), 1),\n",
       " (('w', 'b'), 1),\n",
       " (('p', 'c'), 1),\n",
       " (('h', 'p'), 1),\n",
       " (('f', 'h'), 1),\n",
       " (('b', 'j'), 1),\n",
       " (('f', 'g'), 1),\n",
       " (('z', 'g'), 1),\n",
       " (('c', 'p'), 1),\n",
       " (('p', 'k'), 1),\n",
       " (('p', 'm'), 1),\n",
       " (('x', 'n'), 1),\n",
       " (('s', 'q'), 1),\n",
       " (('k', 'f'), 1),\n",
       " (('m', 'k'), 1),\n",
       " (('x', 'h'), 1),\n",
       " (('g', 'f'), 1),\n",
       " (('v', 'b'), 1),\n",
       " (('j', 'p'), 1),\n",
       " (('g', 'z'), 1),\n",
       " (('v', 'd'), 1),\n",
       " (('d', 'b'), 1),\n",
       " (('v', 'h'), 1),\n",
       " (('h', 'h'), 1),\n",
       " (('g', 'v'), 1),\n",
       " (('d', 'q'), 1),\n",
       " (('x', 'b'), 1),\n",
       " (('w', 'z'), 1),\n",
       " (('h', 'q'), 1),\n",
       " (('j', 'b'), 1),\n",
       " (('x', 'm'), 1),\n",
       " (('w', 'g'), 1),\n",
       " (('t', 'b'), 1),\n",
       " (('z', 'x'), 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(b.items() , key = lambda kv : -kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aafa1e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "N = torch.zeros((27,27), dtype = torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8dbdf66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26,\n",
       " '.': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars  = sorted(list(set(''.join(words))))\n",
    "s_to_i = {s:i+1 for i,s in enumerate(chars)}  # here we are starting from 1 --> a\n",
    "s_to_i['.'] = 0 # here we are gonna rep. the dot with a 0\n",
    "s_to_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "671046b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_to_s = {i:s for s,i in s_to_i.items()}\n",
    "i_to_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bd3ab36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a .\n"
     ]
    }
   ],
   "source": [
    "# create the training set of bigrams (x,y) --- in this we will be given with the first char of the bigram and we are gonna predict the next one \n",
    "\n",
    "xs, ys = [], [] \n",
    "\n",
    "for w in words[:1]:\n",
    "    chs = ['.']+ list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = s_to_i[ch1]\n",
    "        ix2 = s_to_i[ch2]\n",
    "        print(ch1, ch2)\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "# torch.tensor(xs) --> have data type of torch.int64\n",
    "# torch.Tensor(xs) --> have data type of torch.float32\n",
    "# so it is advised to use small t wala tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74a1e2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as saw in the neural network this is the vector of input \n",
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bef190f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as saw in the neural network this is the vector for output corresponding to the input  \n",
    "ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5012b3b5",
   "metadata": {},
   "source": [
    "so from the above eg we get <br>\n",
    "\n",
    "If the input is 0 then the weights gets arranged in such a way that it gives higher probab. to 5<br>\n",
    "\n",
    "and similarly \n",
    "for input 5 it should output 13<br>\n",
    "\n",
    "and for 13 we should have high probability for 13 and 1 cuz' in above eg we are getting both 13 and 1 as the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58ca90b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs, num_classes = 27).float()\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbfd8cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "456200f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc6130dd950>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "befc378f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e8b8cb99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1407e+00, -3.9390e-01, -1.2466e+00,  3.2812e-01, -1.6429e-01,\n",
       "          2.2755e-01,  1.4857e-01, -1.1879e+00, -1.3418e+00,  1.5816e+00,\n",
       "         -9.7445e-01,  4.3036e-01,  7.2853e-01, -4.8896e-01,  8.0670e-01,\n",
       "          3.6872e-01, -7.9320e-02, -2.0618e-01,  3.8349e-01, -2.3429e-01,\n",
       "         -9.5583e-01, -5.5101e-01,  6.4374e-01,  1.2524e+00, -9.0073e-01,\n",
       "         -1.1895e+00, -6.7911e-01],\n",
       "        [ 7.9991e-01, -3.9723e-01,  9.6029e-01, -5.2726e-01, -1.0260e+00,\n",
       "         -6.1431e-01,  3.7970e-01, -2.0867e+00,  9.3030e-01,  7.0047e-01,\n",
       "          1.3679e+00, -1.2359e+00,  2.9912e+00,  5.1322e-01,  9.1668e-01,\n",
       "         -7.5184e-01, -7.4688e-01,  1.0946e+00, -1.0152e+00, -1.7506e-01,\n",
       "          3.1249e-01,  2.9969e-01,  8.7863e-01, -2.9870e-01, -1.6031e-03,\n",
       "          5.1112e-01,  1.5860e+00],\n",
       "        [ 6.2858e-01,  1.2421e+00,  1.2214e+00,  1.3610e+00, -1.4148e+00,\n",
       "          1.4137e+00, -9.6227e-01, -4.3278e-01,  7.3170e-01, -4.8595e-01,\n",
       "          1.1564e+00,  4.8767e-01, -4.6757e-01, -8.3208e-01,  1.7726e+00,\n",
       "          7.3729e-01, -1.1980e-01,  9.2518e-01, -1.6364e-01,  3.6931e-01,\n",
       "          1.7985e+00,  4.2054e-01, -7.0246e-01,  1.2035e-01, -1.3889e+00,\n",
       "         -4.5767e-01, -1.3726e-01],\n",
       "        [ 6.2858e-01,  1.2421e+00,  1.2214e+00,  1.3610e+00, -1.4148e+00,\n",
       "          1.4137e+00, -9.6227e-01, -4.3278e-01,  7.3170e-01, -4.8595e-01,\n",
       "          1.1564e+00,  4.8767e-01, -4.6757e-01, -8.3208e-01,  1.7726e+00,\n",
       "          7.3729e-01, -1.1980e-01,  9.2518e-01, -1.6364e-01,  3.6931e-01,\n",
       "          1.7985e+00,  4.2054e-01, -7.0246e-01,  1.2035e-01, -1.3889e+00,\n",
       "         -4.5767e-01, -1.3726e-01],\n",
       "        [ 4.3731e-01, -9.2432e-01, -3.3657e-01, -8.9203e-01, -1.8913e+00,\n",
       "         -5.4790e-01, -1.2384e+00,  1.2378e+00, -1.5631e+00, -2.4488e-01,\n",
       "          3.6769e-01,  1.4579e+00, -1.3468e+00, -1.9490e-01,  9.0780e-01,\n",
       "          1.9250e+00,  1.7726e+00,  3.6697e-01,  6.9239e-01, -1.8550e+00,\n",
       "         -1.7797e+00,  3.6725e-01, -2.5841e-01,  5.0528e-01,  8.9968e-01,\n",
       "          2.0595e-01,  5.7884e-01]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn((27, 27)) #-->it returns a tensor filled with random numbers from a notmal distribution with mean 0 and variance 1\n",
    "xenc @ W # @---> is the matrix multiplication operator in pytorch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3b6aaac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xenc@ W).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "33046868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3196,  0.6744,  0.2875,  1.3884,  0.8485,  1.2555,  1.1602,  0.3049,\n",
       "          0.2614,  4.8629,  0.3774,  1.5378,  2.0720,  0.6133,  2.2405,  1.4459,\n",
       "          0.9237,  0.8137,  1.4674,  0.7911,  0.3845,  0.5764,  1.9036,  3.4988,\n",
       "          0.4063,  0.3044,  0.5071],\n",
       "        [ 2.2253,  0.6722,  2.6125,  0.5902,  0.3584,  0.5410,  1.4618,  0.1241,\n",
       "          2.5353,  2.0147,  3.9273,  0.2906, 19.9097,  1.6707,  2.5010,  0.4715,\n",
       "          0.4738,  2.9880,  0.3623,  0.8394,  1.3668,  1.3494,  2.4076,  0.7418,\n",
       "          0.9984,  1.6672,  4.8842],\n",
       "        [ 1.8750,  3.4628,  3.3918,  3.9001,  0.2430,  4.1110,  0.3820,  0.6487,\n",
       "          2.0786,  0.6151,  3.1784,  1.6285,  0.6265,  0.4351,  5.8861,  2.0903,\n",
       "          0.8871,  2.5223,  0.8491,  1.4467,  6.0407,  1.5228,  0.4954,  1.1279,\n",
       "          0.2494,  0.6328,  0.8717],\n",
       "        [ 1.8750,  3.4628,  3.3918,  3.9001,  0.2430,  4.1110,  0.3820,  0.6487,\n",
       "          2.0786,  0.6151,  3.1784,  1.6285,  0.6265,  0.4351,  5.8861,  2.0903,\n",
       "          0.8871,  2.5223,  0.8491,  1.4467,  6.0407,  1.5228,  0.4954,  1.1279,\n",
       "          0.2494,  0.6328,  0.8717],\n",
       "        [ 1.5485,  0.3968,  0.7142,  0.4098,  0.1509,  0.5782,  0.2899,  3.4479,\n",
       "          0.2095,  0.7828,  1.4444,  4.2970,  0.2601,  0.8229,  2.4789,  6.8549,\n",
       "          5.8862,  1.4434,  1.9985,  0.1565,  0.1687,  1.4438,  0.7723,  1.6575,\n",
       "          2.4588,  1.2287,  1.7840]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xenc@ W).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "88dc422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5, 27) @ (27 , 27) the dot product will give --> (5, 27) \n",
    "# that (5,27) matrix will give us the log values no the probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d6d1cde6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0102, 0.0216, 0.0092, 0.0445, 0.0272, 0.0402, 0.0372, 0.0098, 0.0084,\n",
       "         0.1557, 0.0121, 0.0492, 0.0664, 0.0196, 0.0717, 0.0463, 0.0296, 0.0261,\n",
       "         0.0470, 0.0253, 0.0123, 0.0185, 0.0610, 0.1120, 0.0130, 0.0097, 0.0162],\n",
       "        [0.0371, 0.0112, 0.0436, 0.0098, 0.0060, 0.0090, 0.0244, 0.0021, 0.0423,\n",
       "         0.0336, 0.0655, 0.0048, 0.3319, 0.0279, 0.0417, 0.0079, 0.0079, 0.0498,\n",
       "         0.0060, 0.0140, 0.0228, 0.0225, 0.0401, 0.0124, 0.0166, 0.0278, 0.0814],\n",
       "        [0.0366, 0.0676, 0.0662, 0.0762, 0.0047, 0.0803, 0.0075, 0.0127, 0.0406,\n",
       "         0.0120, 0.0621, 0.0318, 0.0122, 0.0085, 0.1150, 0.0408, 0.0173, 0.0493,\n",
       "         0.0166, 0.0283, 0.1180, 0.0297, 0.0097, 0.0220, 0.0049, 0.0124, 0.0170],\n",
       "        [0.0366, 0.0676, 0.0662, 0.0762, 0.0047, 0.0803, 0.0075, 0.0127, 0.0406,\n",
       "         0.0120, 0.0621, 0.0318, 0.0122, 0.0085, 0.1150, 0.0408, 0.0173, 0.0493,\n",
       "         0.0166, 0.0283, 0.1180, 0.0297, 0.0097, 0.0220, 0.0049, 0.0124, 0.0170],\n",
       "        [0.0354, 0.0091, 0.0163, 0.0094, 0.0035, 0.0132, 0.0066, 0.0789, 0.0048,\n",
       "         0.0179, 0.0331, 0.0984, 0.0060, 0.0188, 0.0567, 0.1569, 0.1347, 0.0330,\n",
       "         0.0457, 0.0036, 0.0039, 0.0330, 0.0177, 0.0379, 0.0563, 0.0281, 0.0408]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = xenc @ W # log-counts\n",
    "counts = logits.exp() # equivalent N--> refering to the blue table in the previous notebook\n",
    "probs = counts/ counts.sum(1, keepdims=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "514fe0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0102, 0.0216, 0.0092, 0.0445, 0.0272, 0.0402, 0.0372, 0.0098, 0.0084,\n",
       "        0.1557, 0.0121, 0.0492, 0.0664, 0.0196, 0.0717, 0.0463, 0.0296, 0.0261,\n",
       "        0.0470, 0.0253, 0.0123, 0.0185, 0.0610, 0.1120, 0.0130, 0.0097, 0.0162])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f02b930d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "94d76bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9b2df5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summaryyy-------------------------->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "75b38109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "00a540fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "080572d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly initialize 27 neurons' weights. each neuron recieves 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator = g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bd69af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs, num_classes = 27).float() # input to the network : one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts/ counts.sum(1, keepdims = True) # probab. for next char\n",
    "\n",
    "# the last two lines here together are called as softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f610058d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "44464c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "bigram example 1: .e (indexes 0,5)\n",
      "input to the neural net: 0\n",
      "output probabilities from the neural net: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\n",
      "label (actual next character): 5\n",
      "probability assigned by the net to the the correct character: 0.01228625513613224\n",
      "log likelihood: -4.399273872375488\n",
      "negative log likelihood: 4.399273872375488\n",
      "--------\n",
      "bigram example 2: em (indexes 5,13)\n",
      "input to the neural net: 5\n",
      "output probabilities from the neural net: tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
      "        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
      "        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.018050700426101685\n",
      "log likelihood: -4.014570713043213\n",
      "negative log likelihood: 4.014570713043213\n",
      "--------\n",
      "bigram example 3: mm (indexes 13,13)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.026691533625125885\n",
      "log likelihood: -3.623408794403076\n",
      "negative log likelihood: 3.623408794403076\n",
      "--------\n",
      "bigram example 4: ma (indexes 13,1)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label (actual next character): 1\n",
      "probability assigned by the net to the the correct character: 0.07367686182260513\n",
      "log likelihood: -2.6080665588378906\n",
      "negative log likelihood: 2.6080665588378906\n",
      "--------\n",
      "bigram example 5: a. (indexes 1,0)\n",
      "input to the neural net: 1\n",
      "output probabilities from the neural net: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\n",
      "label (actual next character): 0\n",
      "probability assigned by the net to the the correct character: 0.014977526850998402\n",
      "log likelihood: -4.201204299926758\n",
      "negative log likelihood: 4.201204299926758\n",
      "=========\n",
      "average negative log likelihood, i.e. loss = 3.7693049907684326\n"
     ]
    }
   ],
   "source": [
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "  # i-th bigram:\n",
    "  x = xs[i].item() # input character index\n",
    "  y = ys[i].item() # label character index\n",
    "  print('--------')\n",
    "  print(f'bigram example {i+1}: {i_to_s[x]}{i_to_s[y]} (indexes {x},{y})')\n",
    "  print('input to the neural net:', x)\n",
    "  print('output probabilities from the neural net:', probs[i])\n",
    "  print('label (actual next character):', y)\n",
    "  p = probs[i, y]\n",
    "  print('probability assigned by the net to the the correct character:', p.item())\n",
    "  logp = torch.log(p)\n",
    "  print('log likelihood:', logp.item())\n",
    "  nll = -logp\n",
    "  print('negative log likelihood:', nll.item())\n",
    "  nlls[i] = nll\n",
    "\n",
    "print('=========')\n",
    "print('average negative log likelihood, i.e. loss =', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ba4d19e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------!!!Optimization!!!------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a31a5467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "520b180c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8c25a8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly initialize 27 neurons' weights. each neuron recieves 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator = g, requires_grad= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2b4f8a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass \n",
    "xenc = F.one_hot(xs, num_classes= 27).float() # input to the network: one- hot encoding\n",
    "logits = xenc @W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims = True) # probab. for next character\n",
    "loss = -probs[torch.arange(5), ys].log().mean() # here we have implemented the loss as forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d3424d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "339ef741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0123, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0181, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0267, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0737, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0150, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as we are doing classification that why we are using negative log_likelihood instead to mean squared error \n",
    "#([     5,          13,          13,          1,          0])\n",
    "probs[0,5], probs[1,13], probs[2,13], probs[3,1], probs[4,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d133ed85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ae5616b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7693, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3151369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass -----------\n",
    "W.grad = None # set to zero the gradient\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "109f925a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 27])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1ba61027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0121,  0.0020,  0.0025,  0.0008,  0.0034, -0.1975,  0.0005,  0.0046,\n",
       "          0.0027,  0.0063,  0.0016,  0.0056,  0.0018,  0.0016,  0.0100,  0.0476,\n",
       "          0.0121,  0.0005,  0.0050,  0.0011,  0.0068,  0.0022,  0.0006,  0.0040,\n",
       "          0.0024,  0.0307,  0.0292],\n",
       "        [-0.1970,  0.0017,  0.0079,  0.0020,  0.0121,  0.0062,  0.0217,  0.0026,\n",
       "          0.0025,  0.0010,  0.0205,  0.0017,  0.0198,  0.0022,  0.0046,  0.0041,\n",
       "          0.0082,  0.0016,  0.0180,  0.0106,  0.0093,  0.0062,  0.0010,  0.0066,\n",
       "          0.0131,  0.0101,  0.0018],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0058,  0.0159,  0.0050,  0.0104,  0.0398,  0.0058,  0.0019,  0.0067,\n",
       "          0.0019,  0.0060,  0.0140,  0.0046,  0.0023, -0.1964,  0.0022,  0.0063,\n",
       "          0.0058,  0.0009,  0.0183,  0.0043,  0.0097,  0.0060,  0.0100,  0.0005,\n",
       "          0.0024,  0.0004,  0.0094],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0125, -0.1705,  0.0194,  0.0133,  0.0270,  0.0080,  0.0105,  0.0100,\n",
       "          0.0490,  0.0066,  0.0030,  0.0316,  0.0052, -0.1893,  0.0059,  0.0045,\n",
       "          0.0234,  0.0049,  0.0260,  0.0023,  0.0083,  0.0031,  0.0053,  0.0081,\n",
       "          0.0482,  0.0187,  0.0051],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "33b9d10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update-----------\n",
    "W.data += -0.1 *W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c869378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25e9b1a9",
   "metadata": {},
   "source": [
    "<h1> FINAL CODE</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "051999ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = s_to_i[ch1]\n",
    "    ix2 = s_to_i[ch2]\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "da96fe00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.511415481567383\n",
      "2.4843733310699463\n",
      "2.482968330383301\n",
      "2.4828999042510986\n",
      "2.482862710952759\n",
      "2.482832670211792\n",
      "2.4828059673309326\n",
      "2.482780694961548\n",
      "2.4827563762664795\n",
      "2.4827325344085693\n",
      "2.482710123062134\n",
      "2.48268723487854\n",
      "2.4826653003692627\n",
      "2.4826438426971436\n",
      "2.4826221466064453\n",
      "2.4826014041900635\n",
      "2.4825804233551025\n",
      "2.482560157775879\n",
      "2.4825398921966553\n",
      "2.4825198650360107\n",
      "2.4825003147125244\n",
      "2.482481002807617\n",
      "2.48246169090271\n",
      "2.482442855834961\n",
      "2.482424020767212\n",
      "2.482405424118042\n",
      "2.4823873043060303\n",
      "2.4823696613311768\n",
      "2.4823520183563232\n",
      "2.4823343753814697\n",
      "2.4823169708251953\n",
      "2.482300043106079\n",
      "2.482283115386963\n",
      "2.482266902923584\n",
      "2.482250452041626\n",
      "2.482234239578247\n",
      "2.482217788696289\n",
      "2.4822020530700684\n",
      "2.4821865558624268\n",
      "2.482171058654785\n",
      "2.4821557998657227\n",
      "2.4821410179138184\n",
      "2.4821255207061768\n",
      "2.4821112155914307\n",
      "2.4820966720581055\n",
      "2.4820821285247803\n",
      "2.4820680618286133\n",
      "2.4820539951324463\n",
      "2.4820396900177\n",
      "2.4820261001586914\n",
      "2.4820125102996826\n",
      "2.481999397277832\n",
      "2.4819858074188232\n",
      "2.4819726943969727\n",
      "2.481959819793701\n",
      "2.4819469451904297\n",
      "2.481934070587158\n",
      "2.481921672821045\n",
      "2.4819095134735107\n",
      "2.4818971157073975\n",
      "2.481884717941284\n",
      "2.481873035430908\n",
      "2.481860876083374\n",
      "2.481849193572998\n",
      "2.481837749481201\n",
      "2.4818265438079834\n",
      "2.4818148612976074\n",
      "2.4818036556243896\n",
      "2.481792449951172\n",
      "2.4817817211151123\n",
      "2.4817705154418945\n",
      "2.481759786605835\n",
      "2.4817495346069336\n",
      "2.481738805770874\n",
      "2.4817285537719727\n",
      "2.481718063354492\n",
      "2.48170804977417\n",
      "2.4816977977752686\n",
      "2.481687545776367\n",
      "2.481677770614624\n",
      "2.481668472290039\n",
      "2.481658458709717\n",
      "2.4816489219665527\n",
      "2.4816393852233887\n",
      "2.481630325317383\n",
      "2.481621026992798\n",
      "2.481611728668213\n",
      "2.481602668762207\n",
      "2.481593608856201\n",
      "2.4815847873687744\n",
      "2.4815759658813477\n",
      "2.4815673828125\n",
      "2.4815585613250732\n",
      "2.4815502166748047\n",
      "2.481541633605957\n",
      "2.4815332889556885\n",
      "2.48152494430542\n",
      "2.4815165996551514\n",
      "2.481508731842041\n",
      "2.4815003871917725\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(100):\n",
    "  \n",
    "  # forward pass\n",
    "  xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
    "  print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -50 * W.grad #in this 50 is the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3b5126bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "janasah.\n",
      "p.\n",
      "cfay.\n",
      "a.\n"
     ]
    }
   ],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "  \n",
    "  out = []\n",
    "  ix = 0\n",
    "  while True:\n",
    "    \n",
    "    # ----------\n",
    "    # BEFORE:\n",
    "    #p = P[ix]\n",
    "    # ----------\n",
    "    # NOW:\n",
    "    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # ----------\n",
    "    \n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(i_to_s[ix])\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f03b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
